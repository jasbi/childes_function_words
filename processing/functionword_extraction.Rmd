---
title: "Extracting Function Words of Interest from Childes"
author: "Masoud Jasbi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(childesr)
library(tidyverse)
library(feather)
```

# Loading Data from Childes-DB

We use the package `r childesr` to access the data in the CHILDES database. The code below downloades and stores all utterances by children and parents. Utterances are annotated for the type of sentence (declarative, interrogative, imperative).

```{r ChildesDBimports}
#Getting data from 1270 children in 73 corpora...
all_english_tokens <- get_tokens(collection = c("Eng-NA","Eng-UK"), 
                          role = c("target_child","Mother", "Father"),
                          token = "*")

# Import statistics on the speakers in CHILDES
#speaker_stats <- get_speaker_statistics(collection = c("Eng-NA","Eng-UK"), 
#                                        role = c("target_child","Mother", "Father"))
```

# Token Processing

## Exclusions

The following script cleans up the data to exclude: unintelligible tokens and tokens above 72 months of the child's age. All exclusions are stored in a file called exclusions.csv

>>>> Aaron: Edit the code below to exclude babbling, count the number of babbling tokens and report, also exclude unreasonable early productions if you can argue there are any 


```{r exclusionsTokens}
# count the tokens before exclusions
initial <- nrow(all_english_tokens)

# number of children before exclusions
n_chi_initial <-
  all_english_tokens$target_child_id %>% unique() %>% length()

# remove the unintelligible tokens
english_tokens <- 
  all_english_tokens %>% 
  filter(gloss!="xxx", gloss!="xx", gloss!="yyy", gloss!="www", gloss!="zzz")

# count the tokens after excluding unintelligible ones
unintels <- nrow(english_tokens)

# number of children after excluding unintelligible tokens
n_chi_unintels <-
  english_tokens$target_child_id %>% unique() %>% length()

## babbling ##

# remove babbles
english_tokens <-
  all_english_tokens %>%
  filter(part_of_speech != "bab")

# babbles that slipped by 
babbles <- c("uh", "ah", "ooh", "haha", "mm", "mmagh", "ha", "iy", "eh", "aw", "uh", "wahhh", "wahhhh", "baba", "e", "hm", "huh", "mhm","kh", "ne", "ay", "hah", "oh", "um", "va", "ew", "ih", "gh", "rh", "agh", "hah", "uhm", "oo", "heh", "wah", "uhkuh", "mmmmmmamm", "ua", "uhuh", "mmmmbghehmmbgg", "mmbmm", "ummggmm", "mmg", "mmhh", "mmmum", "mmbgeh", "mmmhm", "mmmbg", "ea", "hu", "mmmmbg", "uhoh", "baa", "la", "mmuahah", "muhooh", "uuhooah", "uheh", "gah","hagh", "mmnanananana", "unhunh", "mmbgmm", "ugh", "bowbow", "bowbowbow", "naynay", "ba", "gi", "ado", "Ada", "adaa", "doo", "lalala", "uhhuh", "ummhm", "Dwww's", "Jwww", "Jwww's", "Lala", "ee_ay", "dadadado", "mmmmg", "mmbg", "uhoh")

# remove all glosses that are just a letter, except for A, a, I, and i
alphabet <- c(LETTERS, letters)  # Combine uppercase and lowercase letters
# filtered_alphabet <- alphabet[!alphabet %in% c("A", "I", "a", "i")]

#filter out babbles
english_tokens <- 
  all_english_tokens %>%
  filter(!gloss %in% babbles)


# remove other unlikely words/transcrption mistakes
#vector of ids
unreasonable_id <- c(10927346, 10937151, 11379328, 11382701, 10972379, 11030373, 11030647, 4030824, 10114202)
#first three are "vocalizes" or "vocalizing", "classic", "vocalizes", "references", "transcribed"


english_tokens <- 
  all_english_tokens %>%
  filter(!id %in% unreasonable_id)

# count the tokens after excluding babbles and unreasonable words
babble_unreasonable <- nrow(english_tokens)

# number of children after excluding babbles/unreasonable tokens
n_chi_babble_unreason <-
  english_tokens$target_child_id %>% unique() %>% length()

# remove NAs target_child_age
english_tokens <- 
  english_tokens %>% drop_na(target_child_age)

# count the tokens after removing NA tokens
nas <- nrow(english_tokens)

# number of children after excluding NAs
n_chi_nas <-
  english_tokens$target_child_id %>% unique() %>% length()

#Take out data for the age range above 6 years
english_tokens <-
  english_tokens %>%
  filter(target_child_age < 72)

# count the tokens after excluding the below 1 and older than 6 age range
age_ex <- nrow(english_tokens)

# number of children left after exclusions
n_chi_age <-
  english_tokens$target_child_id %>% unique() %>% length()

# record the dataframe of exclusions
exclusions <-
  data.frame (
    initial = initial,
    after_unintels = unintels,
    after_nas = nas,
    after_age = age_ex,
    unintelligible = initial - unintels,
    missing = unintels - nas,
    age = nas - age_ex,
    n_chi_total = n_chi_initial,
    n_chi_unintels = n_chi_unintels,
    n_chi_nas = n_chi_nas,
    n_chi_age = n_chi_age)
```

```{r savingData}
# save the exclusion data in a file as well as the final data
write_csv(exclusions, "../raw_data/token_exclusions.csv")
write_feather(english_tokens, "../raw_data/english_tokens.feather")
#write_csv(speaker_stats, "../raw_data/speaker_stats.csv")
```

## Coding Speaker Roles

Here we group mothers and fathers together as "parents".

```{r}
# Collapse mothers and fathers into parents
english_tokens$speaker <- "parent"
english_tokens$speaker[english_tokens$speaker_role=="Target_Child"] <- "child"
```

## Coding Age

Here we bin the age data per month.

```{r age}
english_tokens$age <- english_tokens$target_child_age %>% floor()
```

## Grouping Utterance Types

Grouping utterances as declarative, imperative, interrogative, and other.

```{r utterance_types}
# Prepare the utterance_type categories for this study based on the utterance_types in childes-db
## Categories: declarative, impertaive, interrogative, and other
english_tokens$utterance_type <-
  recode(english_tokens$utterance_type, 
         question = "interrogative",
         `broken for coding`="other",
          `imperative_emphatic` = "imperative",
         interruption = "other",
         `interruption question` = "interrogative",
         `missing CA terminator` = "other",
         `no break TCU continuation` = "other",
         `question exclamation` = "interrogative",
         `quotation next line` = "other",
         `quotation precedes` = "other",
         `self interruption` = "other",
         `self interruption question` = "interrogative",
         `trail off` = "other",
         `trail off question` = "interrogative"
         )
```

## Removing Case Sensitivity

we should make sure all words are in lower case:

```{r lowerCase}
english_tokens$gloss <- english_tokens$gloss %>% tolower()
```

## Contractions

>>> Aaron

```{r}
# Define the patterns and exceptions
patterns <- c("n't", "'ll", "'s", "'d", "'m")
exceptions <- c("can't", "won't", "cannot")

process_words <- function(df, patterns, exceptions) {
  new_df <- data.frame()  # Initialize an empty data frame
  
  for (i in 1:nrow(df)) {
    row_i <- df[i, ]
    matched_pattern <- NA  # To store a found pattern, if any
    
    # Check if the gloss is an exception
    if(row_i$gloss %in% exceptions) {
      if(row_i$gloss == "can't") {
        # Remove only "'t" from "can't" to leave "can"
        row_i$gloss <- sub("'t$", "", row_i$gloss)
        matched_pattern <- "n't"
      } 
      else if (row_i$gloss == "cannot"){
        # remove not, leave can
        row_i$gloss <- sub("not$", "", row_i$gloss)
        matched_pattern <- "not"
      }
      else if (row_i$gloss == "won't") {
        # For "won't", do nothing and leave it unchanged
        matched_pattern <- NA
      }
    } else {
      # Process normally: check if gloss ends with any of the specified patterns
      for (p in patterns) {
        if (grepl(paste0(p, "$"), row_i$gloss)) {
          row_i$gloss <- sub(paste0(p, "$"), "", row_i$gloss)
          matched_pattern <- p
          break  # Stop after the first matching pattern
        }
      }
    }
    
    # Append the (possibly modified) original row
    new_df <- rbind(new_df, row_i)
    
    # If a pattern was found (or set in the exception), add a new row beneath with that pattern
    if (!is.na(matched_pattern)) {
      new_row <- df[i, ]
      new_row$gloss <- matched_pattern
      new_df <- rbind(new_df, new_row)
    }
  }
  
  rownames(new_df) <- NULL  # Reset row names
  return(new_df)
}

# test_english_tokens <- english_tokens %>% slice(5000:7000)

contraction_tokens <- process_words(test_english_tokens, patterns, exceptions)

```

Also make sure to separate "cannot" into "can" and "not". Make sure all types of negation contractions are stored as "nt". Make sure "won't" remains as one token because it is not really a contraction of will+not. 

# Defining Function Words in English

How do we define what function words are?

Here is a list of our function words:
* Logical Connectives: no, not, n't (contracted not), and, or, if, nor, therefore
* Quantifiers: none, some, each, every, all, most, few, many, several, a few, both, everyone, someone, somebody, everybody, noone, everything, something, nowhere, somewhere, everywhere
* Comparative Quantity: more, less, much
* Numerals: 
  1. numbers: one, two, three, four, five, six, seven, eight, nine, ten
  2. ordinals: first, second, third, fourth, fifth, last
* Modals: can, could, need, may, might, should, ought, must, maybe, perhaps, shall, will, would
* Negative Polarity Items: any, anyone, anything, anyhow, anywhere, anything, anyway, ever, yet
* Definiteness and Demonstratives:  
  1. Definites: the, a, an
  2. Demonstratives: this, that, these, those
* Temporals: 
  1. Quantifiers: always, usually, seldom, never, sometimes, now, often, once 
  2. Connectives: when, while, after, before, then, until, since, whenever, during
* Interrogatives: who, when, what, whose, where, how, why, whom
* Locatives: on, in, out, up, down, under, above, below, along, over, behind, across, beside, between, beyond, inside, outside, into, near, onto, toward
* Causal Connectives: because
* Contrast Connectives: but, although
* Additives: again, too, also, another, other, others, still
* Exclusives: only, just
* Emphasis: even, indeed
* Auxiliaries: am, is, are, do, does
* Pronouns: I, you, ..
* Other: either, neither, whether as, else, almost, already, except, for, from, instead, such, with, without, about, by, very, to

Then we create a word column that marks all words as "other". Then we mark the function words of interest.

```{r NLTK list}
c('couldn', "couldn't",
 "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shouldn', "shouldn't", "won't", 'wouldn', "wouldn't")
```

```{r FunctionWords}
english_tokens$word <- "content"

function_words <- c("no", "not", "nt", "yes", "and", "or", "if", "nor", "therefore", "none", "some", "each", "every", "all", "most", "few", "many", "several", "few", "both", "everyone", "someone", "somebody", "everybody", "nonone", "everything", "something", "nowhere", "somewhere", "everywhere", "more", "less", "much", "most", "least", "than", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "nineth", "tenth", "last", "can", "could", "need", "may", "might", "should", "ought", "must", "maybe", "perhaps", "shall", "will", "would", "won't", "any", "anyone", "anything", "anywhere", "anything", "anyway", "anyways", "ever", "yet", "the", "a", "an", "this", "that", "these", "those", "always", "usually", "seldom", "never", "sometimes", "often", "once", "twice", "now", "while", "after", "before", "then", "until", "since", "whenever", "during", "who", "when", "what", "whose", "where", "how", "why", "whom", "on", "in", "out", "up", "down", "under", "above", "below", "along", "over", "behind", "across", "beside", "between", "beyond", "into", "near", "onto", "toward", "here", "through", "here", "there", "because", "but", "although", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing", "i", "you", "we", "he", "she", "they", "me", "us", "her", "him", "them", "my", "your", "our", "his", "their", "its", "mine", "yours", "ours", "hers", "theirs", "myself", "yourself", "ourselves", "himself", "herself", "yourselves", "themselves", "it", "itself", "again", "too", "also", "another", "other", "others", "still", "only", "just", "even", "indeed", "either", "neither", "whether", "as", "else", "almost", "already", "except", "for", "from", "instead", "same", "different", "such", "with", "without", "about", "by", "very", "unless", "to", "of", "would", "at", "against")

for (x in function_words){
  english_tokens$word[english_tokens$gloss==x] <- x
}
```

First we start with marking content words and selecting logical connectives:

```{r logicalConnectives}
english_tokens$word_category <- "content"

logical_connectives <- c("no", "not", "nt", "and", "or", "if", "nor", "therefore") 

for (x in logical_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Logicals"
}
```

Labeling Quantifiers:

```{r quantifiers}
quantifiers <- c("none", "some", "each", "every", "all", "most", "few", "many", "several", "few", "both", "everyone", "someone", "somebody", "everybody", "nonone", "everything", "something", "nowhere", "somewhere", "everywhere") 

for (x in quantifiers){
  english_tokens$word_category[english_tokens$gloss==x] <- "Quantifiers"
}
```

Comparative, superlative, and quantity words:

```{r quantity}
quantity <- c("more", "less", "much", "most", "least", "than") 

for (x in quantity){
  english_tokens$word_category[english_tokens$gloss==x] <- "Quantity"
}
```

Cardinal Numbers:

```{r cardinals}
cardinals <- c("one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten") 

for (x in cardinals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Cardinals"
}
```

Ordinal Numbers: 

```{r ordinals}
ordinals <- c("first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "nineth", "tenth", "last") 

for (x in ordinals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Ordinals"
}
```

Modals:

```{r modals}
modals <- c("can", "could", "need", "may", "might", "should", "ought", "must", "maybe", "perhaps", "shall", "will", "would", "won't") 

for (x in modals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Modals"
}
```

Negative Polarity Items:

```{r NPIs}
NPI <- c("any", "anyone", "anything", "anywhere", "anything", "anyway", "anyways", "ever", "yet") 

for (x in NPI){
  english_tokens$word_category[english_tokens$gloss==x] <- "NPI"
}
```

Definites:

```{r definiteness}
definites <- c("the", "a", "an") 

for (x in definites){
  english_tokens$word_category[english_tokens$gloss==x] <- "Definites"
}
```

Demonstratives:

```{r demonstratives}
# "that" is ambiguous between the complementizer and the determiner
demonstratives <- c("this", "that", "these", "those") 

for (x in demonstratives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Demonstratives"
}
```

Temporal Quantifiers and Adverbs:

```{r temporalAdverbs}
temporal_quantifiers <- c("always", "usually", "seldom", "never", "sometimes", "often", "once", "twice", "now") 

for (x in temporal_quantifiers){
  english_tokens$word_category[english_tokens$gloss==x] <- "Temporal Quantifiers"
}
```

Temporal Connectives and Prepositions:

```{r temporalConnectives}
temporal_connectives <- c("while", "after", "before", "then", "until", "since", "whenever", "during") 

for (x in temporal_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Temporal Connectives"
}
```

Wh words:

```{r WHwords}
Wh <- c("who", "when", "what", "whose", "where", "how", "why", "whom") 

for (x in Wh){
  english_tokens$word_category[english_tokens$gloss==x] <- "Wh"
}
```

Locatives:

```{r Locatives}
locatives <- c("on", "in", "out", "up", "down", "under", "above", "below", "along", "over", "behind", "across", "beside", "between", "beyond", "into", "near", "onto", "toward", "here", "through", "here", "there") 

for (x in locatives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Locatives"
}
```

Causal Connectives:

```{r causals}
causatives <- c("because") 

for (x in causatives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Causal Connectives"
}
```

Contrast Connectives:

```{r contrastivs}
contrasts <- c("but", "although") 

for (x in contrasts){
  english_tokens$word_category[english_tokens$gloss==x] <- "Contrast Connectives"
}
```

Auxiliaries:

Not all of these verbs are only auxiliary verbs.

```{r auxiliaries}
auxiliaries <- c("am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do", "does", "did", "doing") 

for (x in contrasts){
  english_tokens$word_category[english_tokens$gloss==x] <- "auxiliaries"
}
```

Pronouns:

```{r pronouns}
pronouns <- c("i", "you", "we", "he", "she", "they", "me", "us", "her", "him", "them", "my", "your", , "our", "his", "their", "its", "mine", "yours", "ours", "hers", "theirs", "myself", "yourself", "ourselves", "himself", "herself", "yourselves", "themselves", "it", "itself") 

for (x in pronouns){
  english_tokens$word_category[english_tokens$gloss==x] <- "Pronouns"
}
```

Additives:

```{r additives}
additives <- c("again", "too", "also", "another", "other", "others", "still") 

for (x in temporal_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Additives"
}
```

Exclusives:

```{r exclusives}
exclusives <- c("only","just") 

for (x in exclusives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Exclusives"
}
```

Focus Particles:

```{r focus}
focus <- c("even", "indeed") 

for (x in focus){
  english_tokens$word_category[english_tokens$gloss==x] <- "Focus Particles"
}
```

Other:

```{r}
other <- c("either", "neither", "whether", "as", "else", "almost", "already", "except", "for", "from", "instead", "same", "different", "such", "with", "without", "about", "by", "very", "unless", "to", "of", "would", "at", "against") 

for (x in other){
  english_tokens$word_category[english_tokens$gloss==x] <- "Other"
}
```

```{r}
english_tokens$syncategory <- "Function"
english_tokens$syncategory[english_tokens$word_category=="content"] <- "Content"
```

## Saving Summary Tables

```{r save_dataframe}
write_feather(english_tokens, "../processed_data/english_tokens_processed.feather")
```


# Utterance Processing

## Loading Utterance Data from Childes-DB

We use the package `r childesr` to access the data in the CHILDES database. The code below downloades and stores all utterances by children and parents. Utterances are annotated for the type of sentence (declarative, interrogative, imperative).

```{r ChildesDBimports}
#Import all English utterances from CHILDES 
eng_utterances <- get_utterances(collection = c("Eng-NA","Eng-UK"), 
                                 role = c("target_child","Mother", "Father"))
```


```{r lowerCase}
eng_utterances$utterance <- eng_utterances$gloss %>% tolower()

eng_utterances$no <-
  eng_utterances$utterance %>%
  str_count(pattern = "(^|[^a-z])no([^a-z]|$)")

eng_utterances$not <-
  eng_utterances$utterance %>%
  str_count(pattern = "(^|[^a-z])not([^a-z]|$)")

eng_utterances$nt <-
  eng_utterances$utterance %>%
  str_count(pattern = "n't")
```


