---
title: "Extracting Function Words of Interest from Childes"
author: "Masoud Jasbi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(childesr)
library(tidyverse)
library(feather)
```

# Loading Data from Childes-DB

We use the package `r childesr` to access the data in the CHILDES database. The code below downloades and stores all utterances by children and parents. Utterances are annotated for the type of sentence (declarative, interrogative, imperative).

```{r ChildesDBimports}
#Getting data from 1270 children in 73 corpora...
all_english_tokens <- get_tokens(collection = c("Eng-NA","Eng-UK"), 
                          role = c("target_child","Mother", "Father"),
                          token = "*")

#Import all English utterances from CHILDES 
#eng_utterances <- get_utterances(collection = c("Eng-NA","Eng-UK"), 
#                                 role = c("target_child","Mother", "Father"))

# Import statistics on the speakers in CHILDES
#speaker_stats <- get_speaker_statistics(collection = c("Eng-NA","Eng-UK"), 
#                                        role = c("target_child","Mother", "Father"))

```

# Token Processing

## Exclusions

The following script cleans up the data to exclude: unintelligible tokens and tokens above 72 months of the child's age. All exclusions are stored in a file called exclusions.csv

```{r exclusionsTokens}
# count the tokens before exclusions
initial <- nrow(all_english_tokens)

# number of children before exclusions
n_chi_initial <-
  all_english_tokens$target_child_id %>% unique() %>% length()

# remove the unintelligible tokens
english_tokens <- 
  all_english_tokens %>% 
  filter(gloss!="xxx", gloss!="xx", gloss!="yyy", gloss!="www", gloss!="zzz")

# count the tokens after excluding unintelligible ones
unintels <- nrow(english_tokens)

# number of children after excluding unintelligible tokens
n_chi_unintels <-
  english_tokens$target_child_id %>% unique() %>% length()

# remove NAs target_child_age
english_tokens <- 
  english_tokens %>% drop_na(target_child_age)

# count the tokens after removing NA tokens
nas <- nrow(english_tokens)

# number of children after excluding NAs
n_chi_nas <-
  english_tokens$target_child_id %>% unique() %>% length()

#Take out data for the age range above 6 years
english_tokens <-
  english_tokens %>%
  filter(target_child_age < 72)

# count the tokens after excluding the below 1 and older than 6 age range
age_ex <- nrow(english_tokens)

# number of children left after exclusions
n_chi_age <-
  english_tokens$target_child_id %>% unique() %>% length()

# record the dataframe of exclusions
exclusions <-
  data.frame (
    initial = initial,
    after_unintels = unintels,
    after_nas = nas,
    after_age = age_ex,
    unintelligible = initial - unintels,
    missing = unintels - nas,
    age = nas - age_ex,
    n_chi_total = n_chi_initial,
    n_chi_unintels = n_chi_unintels,
    n_chi_nas = n_chi_nas,
    n_chi_age = n_chi_age)
```

```{r savingData}
# save the exclusion data in a file as well as the final data
write_csv(exclusions, "../raw_data/token_exclusions.csv")
write_feather(english_tokens, "../raw_data/english_tokens.feather")
#write_csv(speaker_stats, "../raw_data/speaker_stats.csv")
```

## Coding Speaker Roles

Here we group mothers and fathers together as "parents".

```{r}
# Collapse mothers and fathers into parents
english_tokens$speaker <- "parent"
english_tokens$speaker[english_tokens$speaker_role=="Target_Child"] <- "child"
```

## Coding Age

Here we bin the age data per month.

```{r age}
english_tokens$age <- english_tokens$target_child_age %>% floor()
```

## Grouping Utterance Types

Grouping utterances as declarative, imperative, interrogative, and other.

```{r utterance_types}
# Prepare the utterance_type categories for this study based on the utterance_types in childes-db
## Categories: declarative, impertaive, interrogative, and other
english_tokens$utterance_type <-
  recode(english_tokens$utterance_type, 
         question = "interrogative",
         `broken for coding`="other",
          `imperative_emphatic` = "imperative",
         interruption = "other",
         `interruption question` = "interrogative",
         `missing CA terminator` = "other",
         `no break TCU continuation` = "other",
         `question exclamation` = "interrogative",
         `quotation next line` = "other",
         `quotation precedes` = "other",
         `self interruption` = "other",
         `self interruption question` = "interrogative",
         `trail off` = "other",
         `trail off question` = "interrogative"
         )
```

## Removing Case Sensitivity

we should make sure all words are in lower case:

```{r lowerCase}
english_tokens$gloss <- english_tokens$gloss %>% tolower()
```

# Defining Function Words in English

How do we define what function words are?

Here is a list of our function words:
* Logical Connectives: no, not, n't, and, or, if, nor, therefore
* Quantifiers: none, some, each, every, all, most, few, many, several, a few, both, everyone, someone, somebody, everybody, noone, everything, something, nowhere, somewhere, everywhere
* Comparative Quantity: more, less, much
* Numerals: 
  1. numbers: one, two, three, four, five, six, seven, eight, nine, ten
  2. ordinals: first, second, third, fourth fifth, last
* Modals: can, could, need, may, might, should, ought, must, maybe, perhaps, shall, will, would
* Negative Polarity Items: any, anyone, anything, anyhow, anywhere, anything, anyway, ever, yet
* Definiteness and Demonstratives:  
  1. Definites: the, a, an
  2. Demonstratives: this, that, these, those
* Temporals: 
  1. Quantifiers: always, usually, seldom, never, sometimes, now, often, once 
  2. Connectives: when, while, after, before, then, until, since, whenever, during
* Interrogatives: who, when, what, whose, where, how, why, whom
* Locatives: on, in, out, up, down, under, above, below, along, over, behind, across, beside, between, beyond, inside, outside, into, near, onto, toward
* Causal Connectives: because
* Contrast Connectives: but, although
* Additives: again, too, also, another, other, others, still
* Exclusives: only, just
* Emphasis: even, indeed
* Auxiliaries: am, is, are, do, does
* Pronouns: I, you, ..
* Other: either, neither, whether as, else, almost, already, except, for, from, instead, such, with, without, about, by, very, to

Then we create a word column that marks all words as "other". Then we mark the function words of interest.

```{r stopwords}
library(stopwords)
stopwords(language = "en", source = "snowball")
```

```{r NLTK list}
c('i', 'me', 'my', 'myself', 'we', 'our', 'ours', 
'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 
'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 
'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',
 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are',
 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',
 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 
'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 
'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 
'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 
'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 
's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', "aren't", 'couldn', "couldn't", "didn't", "doesn't", 
'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn',
 "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 
'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't") %>% length()
```

```{r NegationWords}
english_tokens$word <- "content"

function_words <- c(
  "no", "not", "yes", "cannot", "ain't", "isn't", "amn't", "aren't", "wasn't", "weren't", "don't", "doesn't", "didn't", "won't", "shan't", "hasn't", "haven't", "hadn't", "shouldn't", "can't", "couldn't", "mayn't", "mightn't", "wouldn't", "mustn't", "don't",
  "and", "or", "if", "nor", "therefore", 
  "none", "some", "each", "every", "all", "most", "few", "many", "several", "few", "both", "everyone", "someone", "somebody", "everybody", "nonone", "everything", "something", "nowhere", "somewhere", "everywhere", 
  "more", "less", "much", 
  "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", 
  "first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "nineth", "tenth", "last", 
  "can", "could", "need", "may", "might", "should", "ought", "must", "maybe", "perhaps", "shall", "will", "would",
  "any", "anyone", "anything", "anywhere", "anything", "anyway", "anyways", "ever", "yet", 
  "the", "a", "an", 
  "this", "that", "these", "those", 
  "always", "usually", "seldom", "never", "sometimes", "often", "once", "twice", 
  "while", "after", "before", "then", "until", "since", "whenever", "during", 
  "who", "when", "what", "whose", "where", "how", "why", "whom", 
  "on", "in", "out", "up", "down", "under", "above", "below", "along", "over", "behind", "across", "beside", "between", "beyond", "inside", "outside", "into", "near", "onto", "toward", "to", "here", "at",
  "because", 
  "but", "although",
  "am", "is", "are",
  "i", "you", "he", "she", "we", "they", "her", "him", "my", "your", "it", "them", "their", "our", "its", "me", "his", "hers", "mine", "yours", "myself", "yourself", "himself", "herself", "itself", "ourselves", "themselves", "yourselves", 
  "i'm", "you're", "he's", "she's", "we're", "they're", 
  "again", "too", "also", "another", "other", "others", "still",
  "only", "just",
  "even", "indeed",
  "either", "neither", "whether", "as", "else", "almost", "already", "except", "for", "from", "instead", "same", "different", "such", "with", "without", "about", "by", "very", "unless",
  "so", "now"
  )

for (x in function_words){
  english_tokens$word[english_tokens$gloss==x] <- x
}
```

First we start with marking content words and selecting logical connectives:

```{r}
english_tokens$word_category <- "content"

logical_connectives <- c("no", "not", "cannot", "ain't", "isn't", "amn't", "aren't", "wasn't", "weren't", "don't", "doesn't", "didn't", "won't", "shan't", "hasn't", "haven't", "hadn't", "shouldn't", "can't", "couldn't", "mayn't", "mightn't", "wouldn't", "mustn't", "and", "or", "if", "nor", "therefore") 

for (x in logical_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Logicals"
}
```

Labeling Quantifiers:

```{r}
quantifiers <- c("none", "some", "each", "every", "all", "most", "few", "many", "several", "few", "both", "everyone", "someone", "somebody", "everybody", "nonone", "everything", "something", "nowhere", "somewhere", "everywhere") 

for (x in quantifiers){
  english_tokens$word_category[english_tokens$gloss==x] <- "Quantifiers"
}
```

Quantity words:

```{r}
quantity <- c("more", "less", "much") 

for (x in quantity){
  english_tokens$word_category[english_tokens$gloss==x] <- "Quantity"
}
```

Cardinal Numbers:

```{r}
cardinals <- c("one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten") 

for (x in cardinals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Cardinals"
}
```

Ordinal Numbers: 

```{r}
ordinals <- c("first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "nineth", "tenth", "last") 

for (x in ordinals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Ordinals"
}
```

Modals:

```{r}
modals <- c("can", "could", "need", "may", "might", "should", "ought", "must", "maybe", "perhaps", "shall", "will", "would") 

for (x in modals){
  english_tokens$word_category[english_tokens$gloss==x] <- "Modals"
}
```

Negative Polarity Items:

```{r}
NPI <- c("any", "anyone", "anything", "anywhere", "anything", "anyway", "anyways", "ever", "yet") 

for (x in NPI){
  english_tokens$word_category[english_tokens$gloss==x] <- "NPI"
}
```

Definites:

```{r}
definites <- c("the", "a", "an") 

for (x in definites){
  english_tokens$word_category[english_tokens$gloss==x] <- "Definites"
}
```

Demonstratives:

```{r}
# "that" is ambiguous between the complementizer and the determiner
demonstratives <- c("this", "that", "these", "those") 

for (x in demonstratives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Demonstratives"
}
```

Temporal Quantifiers:

```{r}
temporal_quantifiers <- c("always", "usually", "seldom", "never", "sometimes", "often", "once", "twice") 

for (x in temporal_quantifiers){
  english_tokens$word_category[english_tokens$gloss==x] <- "Temporal Quantifiers"
}
```

Temporal Connectives:

```{r}
temporal_connectives <- c("while", "after", "before", "then", "until", "since", "whenever", "during") 

for (x in temporal_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Temporal Connectives"
}
```

Wh words:

```{r}
Wh <- c("who", "when", "what", "whose", "where", "how", "why", "whom") 

for (x in Wh){
  english_tokens$word_category[english_tokens$gloss==x] <- "Wh"
}
```

Locatives:

```{r}
locatives <- c("on", "in", "out", "up", "down", "under", "above", "below", "along", "over", "behind", "across", "beside", "between", "beyond", "into", "near", "onto", "toward", "here") 

for (x in locatives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Locatives"
}
```

Causal Connectives:

```{r}
causatives <- c("because") 

for (x in causatives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Causal Connectives"
}
```

Contrast Connectives:

```{r}
contrasts <- c("but", "although") 

for (x in contrasts){
  english_tokens$word_category[english_tokens$gloss==x] <- "Contrast Connectives"
}
```

Auxiliaries:

We did not include "do", "does", "have", and "has" because they are also used as main verbs often

```{r}
auxiliaries <- c("am", "is", "are") 

for (x in contrasts){
  english_tokens$word_category[english_tokens$gloss==x] <- "Be Auxiliaries"
}
```

Pronouns:

```{r}
pronouns <- c("i", "you", "he", "she", "we", "they", "her", "him", "my", "your", "it", "them", "their", "our", "its", "me", "his", "hers", "mine", "yours", "myself", "yourself", "himself", "herself", "itself", "ourselves", "themselves", "yourselves") 

for (x in pronouns){
  english_tokens$word_category[english_tokens$gloss==x] <- "Pronouns"
}
```

Pronouns + auxiliaries:

```{r}
pronoun_aux <- c("i'm", "you're", "he's", "she's", "we're", "they're") 

for (x in pronoun_aux){
  english_tokens$word_category[english_tokens$gloss==x] <- "Pronoun_Aux"
}
```

Additives:

```{r}
additives <- c("again", "too", "also", "another", "other", "others", "still") 

for (x in temporal_connectives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Additives"
}
```

Exclusives:

```{r}
exclusives <- c("only","just") 

for (x in exclusives){
  english_tokens$word_category[english_tokens$gloss==x] <- "Exclusives"
}
```


Focus Particles:

```{r}
focus <- c("even", "indeed") 

for (x in focus){
  english_tokens$word_category[english_tokens$gloss==x] <- "Focus Particles"
}
```

Other:

```{r}
other <- c("either", "neither", "whether", "as", "else", "almost", "already", "except", "for", "from", "instead", "same", "different", "such", "with", "without", "about", "by", "very", "unless", "to", "of") 

for (x in other){
  english_tokens$word_category[english_tokens$gloss==x] <- "Other"
}
```

```{r}
english_tokens$syncategory <- "Function"
english_tokens$syncategory[english_tokens$word_category=="content"] <- "Content"
```

## Saving Summary Tables

```{r save_dataframe}
write_feather(english_tokens, "../processed_data/english_tokens_processed.feather")
```


# Utterance Processing

## Loading Utterance Data from Childes-DB

We use the package `r childesr` to access the data in the CHILDES database. The code below downloades and stores all utterances by children and parents. Utterances are annotated for the type of sentence (declarative, interrogative, imperative).

```{r ChildesDBimports}
#Import all English utterances from CHILDES 
eng_utterances <- get_utterances(collection = c("Eng-NA","Eng-UK"), 
                                 role = c("target_child","Mother", "Father"))
```


```{r lowerCase}
eng_utterances$utterance <- eng_utterances$gloss %>% tolower()

eng_utterances$no <-
  eng_utterances$utterance %>%
  str_count(pattern = "(^|[^a-z])no([^a-z]|$)")

eng_utterances$not <-
  eng_utterances$utterance %>%
  str_count(pattern = "(^|[^a-z])not([^a-z]|$)")

eng_utterances$nt <-
  eng_utterances$utterance %>%
  str_count(pattern = "n't")
```


